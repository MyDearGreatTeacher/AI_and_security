
scikit-learn機器學習：常用演算法原理及程式設計實戰
```
作者:黃永昌 編著
出版日期 :2018-01-26
ISBN:978-7-111-59024-8
```
```
第1章 機器學習介紹	1
1.1 什麼是機器學習	1
　　1.2 機器學習有什麼用	2
　　1.3 機器學習的分類	3
　　1.4 機器學習應用開發的典型步驟	4
1.4.1 資料獲取和標記	4
1.4.2 數據清洗	5
1.4.3 特徵選擇	5
1.4.4 模型選擇	5
1.4.5 模型訓練和測試	5
1.4.6 模型性能評估和優化	5
1.4.7 模型使用	6
　　1.5 複習題	6
 ```
## 第2章 Python機器學習套裝軟體	7
```
2.1 開發環境搭建	7

2.2 IPython簡介	8
2.2.1 IPython基礎	8
2.2.2 IPython圖形介面	13

2.3 Numpy簡介	15
2.3.1 Numpy陣列	15
2.3.2 Numpy運算	19

2.4 Pandas簡介	32
2.4.1 基本資料結構	32
2.4.2 數據排序	34
2.4.3 資料訪問	34
2.4.4 時間序列	36
2.4.5 數據視覺化	36
2.4.6 文件讀寫	38

2.5 Matplotlib簡介	38
2.5.1 圖形樣式	38
2.5.2 圖形物件	40
2.5.3 畫圖操作	46
　　
2.6 scikit-learn簡介	51
2.6.1 scikit-learn示例	51
2.6.2 scikit-learn一般性原理和通用規則	55
　　
2.7 複習題	56
2.8 拓展學習資源	57
```

## 第3章 機器學習理論基礎	58

```
3.1 過擬合和欠擬合	58
　　3.2 成本函數	59
　　3.3 模型準確性	60
3.3.1 模型性能的不同表述方式	61
3.3.2 交叉驗證資料集	61
　　3.4 學習曲線	62
3.4.1 實例：畫出學習曲線	62
3.4.2 過擬合和欠擬合的特徵	65
　　3.5 演算法模型性能優化	65
　　3.6 查準率和召回率	66
　　3.7 F1 Score	67
　　3.8 複習題	67
```
第4章 k-近鄰演算法	69
```
4.1 演算法原理	69
4.1.1 演算法優缺點	69
4.1.2 演算法參數	70
4.1.3 演算法的變種	70
　　4.2 示例：使用k-近鄰演算法進行分類	70
　　4.3 示例：使用k-近鄰演算法進行回歸擬合	72
　　4.4 實例：糖尿病預測	74
4.4.1 載入數據	74
4.4.2 模型比較	75
4.4.3 模型訓練及分析	77
4.4.4 特徵選擇及資料視覺化	78
　　4.5 拓展閱讀	80
4.5.1 如何提高k-近鄰演算法的運算效率	80
4.5.2 相關性測試	80
　　4.6 複習題	81
```
## 第5章 線性回歸演算法	83
```
5.1 演算法原理	83
5.1.1 預測函數	83
5.1.2 成本函數	84
5.1.3 梯度下降演算法	84
　　
5.2 多變數線性回歸演算法	86
5.2.1 預測函數	86
5.2.2 成本函數	87
5.2.3 梯度下降演算法	88

5.3 模型優化	89
5.3.1 多項式與線性回歸	89
5.3.2 數據歸一化	89

5.4 示例：使用線性回歸演算法擬合正弦函數	90

5.5 示例：測算房價	92
5.5.1 輸入特徵	92  5.5.2 模型訓練	93  5.5.3 模型優化	94 5.5.4 學習曲線	95
　　
5.6 拓展閱讀	96
5.6.1 梯度下降反覆運算公式推導	96
5.6.2 隨機梯度下降演算法	96
5.6.3 標準方程	97
　　5.7 複習題	97
```
## 第6章 邏輯回歸演算法	98
```　
  6.1 演算法原理	98
6.1.1 預測函數	98
6.1.2 判定邊界	99
6.1.3 成本函數	100
6.1.4 梯度下降演算法	102
　　6.2 多元分類	102
　　6.3 正則化	103
6.3.1 線性回歸模型正則化	103
6.3.2 邏輯回歸模型正則化	104
　　6.4 演算法參數	104
　　6.5 實例：乳腺癌檢測	106
6.5.1 資料獲取及特徵提取	106
6.5.2 模型訓練	108
6.5.3 模型優化	110
6.5.4 學習曲線	111
　　6.6 拓展閱讀	113
　　6.7 複習題	114
```
## 第7章 決策樹	115
　　
```
7.1 演算法原理	115
7.1.1 信息增益	116
7.1.2 決策樹的創建	119
7.1.3 剪枝演算法	120
　　7.2 演算法參數	121
　　7.3 實例：預測泰坦尼克號倖存者	122
7.3.1 資料分析	122
7.3.2 模型訓練	123
7.3.3 優化模型參數	124
7.3.4 模型參數選擇工具包	127
　　7.4 拓展閱讀	130
7.4.1 熵和條件熵	130
7.4.2 決策樹的構建演算法	130
　　7.5 集合演算法	131
7.5.1 自助聚合演算法Bagging	131
7.5.2 正向激勵演算法boosting	131
7.5.3 隨機森林	132
7.5.4 ExtraTrees演算法	133
　　7.6 複習題	133
```
## 第8章 支持向量機	134
　　
```
8.1 演算法原理	134
8.1.1 大間距分類演算法	134
8.1.2 鬆弛係數	136
　　8.2 核函數	138
8.2.1 最簡單的核函數	138
8.2.2 相似性函數	140
8.2.3 常用的核函數	141
8.2.4 核函數的對比	142
　　8.3 scikit-learn裡的SVM	144
　　8.4 實例：乳腺癌檢測	146
　　8.5 複習題	149
```
## 第9章 樸素貝葉斯演算法	151
　　
```
9.1 演算法原理	151
9.1.1 貝葉斯定理	151
9.1.2 樸素貝葉斯分類法	152
　　9.2 一個簡單的例子	153
　　9.3 概率分佈	154
9.3.1 概率統計的基本概念	154
9.3.2 多項式分佈	155
9.3.3 高斯分佈	158
　　9.4 連續值的處理	159
　　9.5 實例：文檔分類	160
9.5.1 獲取資料集	160
9.5.2 文檔的數學表達	161
9.5.3 模型訓練	163
9.5.4 模型評價	165
　　9.6 複習題	167
```
## 第10章 PCA演算法	168
　　
```
 10.1 演算法原理	168
10.1.1 數據歸一化和縮放	169
10.1.2 計算協方差矩陣的特徵向量	169
10.1.3 資料降維和恢復	170
　　10.2 PCA 演算法示例	171
10.2.1 使用Numpy模擬PCA計算過程	171
10.2.2 使用sklearn進行PCA降維運算	173
10.2.3 PCA的物理含義	174
　　10.3 PCA 的資料還原率及應用	175
10.3.1 資料還原率	175
10.3.2 加快監督機器學習演算法的運算速度	176
　　10.4 實例：人臉識別	176
10.4.1 載入資料集	176
10.4.2 一次失敗的嘗試	179
10.4.3 使用PCA來處理資料集	182
10.4.4 最終結果	185
　　10.5 拓展閱讀	189
　　10.6 複習題	189
  ```
## 第11章 k-均值演算法	190
　　
```
  11.1 演算法原理	190
11.1.1 k-均值演算法成本函數	191
11.1.2 隨機初始化聚類中心點	191
11.1.3 選擇聚類的個數	192
　　11.2 scikit-learn裡的k-均值演算法	192
　　11.3 使用k-均值對文檔進行聚類分析	195
11.3.1 準備資料集	195
11.3.2 載入資料集	196
11.3.3 文本聚類分析	197
　　11.4 聚類演算法性能評估	200
11.4.1 Adjust Rand Index	200
11.4.2 齊次性和完整性	201
11.4.3 輪廓係數	203
　　11.5 複習題	204
　　後記	205
```
